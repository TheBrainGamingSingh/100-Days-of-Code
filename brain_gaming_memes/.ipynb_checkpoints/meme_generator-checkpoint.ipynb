{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "# Import Keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Dropout, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def map_char_to_int(texts):\n",
    "    char_counts = {}\n",
    "    for text in texts:\n",
    "        for char in text:\n",
    "            char_counts[char] = char_counts[char] + 1 if char in char_counts else 1\n",
    "    char_counts_sorted = sorted(char_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    char_to_int = {}\n",
    "    for i, row in enumerate(char_counts_sorted):\n",
    "        char_to_int[row[0]] = i + 1\n",
    "    return char_to_int\n",
    "\n",
    "def texts_to_sequences(texts, char_to_int):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequences.append([char_to_int[char] for char in text])\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models directory already exists...\n",
      "Number of memes: 93452\n",
      "Rows to process: 4466470\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "\n",
    "BASE_PATH = os.getcwd()\n",
    "MODEL_NAME = 'brain_gaming_memes'\n",
    "\n",
    "try: \n",
    "    os.mkdir('models')\n",
    "except FileExistsError:\n",
    "    print('models directory already exists...')\n",
    "\n",
    "MODEL_PATH = BASE_PATH + '/models/' + MODEL_NAME + '_' + datetime.datetime.today().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "os.mkdir(MODEL_PATH)\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 16\n",
    "ROWS_TO_SCAN = 100000\n",
    "NUM_EPOCHS = 48\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Generate input for the model\n",
    "# Read Training Data\n",
    "with open('training_data.json') as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "print(f'Number of memes: {len(training_data)}')\n",
    "\n",
    "texts = []  \n",
    "labels_index = {}  \n",
    "labels = []  \n",
    "label_id_counter = 0\n",
    "\n",
    "for i, row in enumerate(training_data):\n",
    "    template_id = str(row[0]).zfill(12)\n",
    "    text = row[1].lower()\n",
    "    start_index = len(template_id) + 2 + 1 + 2  # template_id, spaces, box_index, spaces\n",
    "    box_index = 0\n",
    "    \n",
    "    for j in range(0, len(text)):\n",
    "        char = text[j]\n",
    "        # note: it is critical that the number of spaces plus len(box_index) is >= the convolution width\n",
    "        texts.append(template_id + '  ' + str(box_index) + '  ' + text[0:j])\n",
    "        if char in labels_index:\n",
    "            label_id = labels_index[char]\n",
    "        else:\n",
    "            label_id = label_id_counter\n",
    "            labels_index[char] = label_id\n",
    "            label_id_counter += 1\n",
    "        labels.append(label_id)\n",
    "        if char == '|':\n",
    "            box_index += 1\n",
    "\n",
    "    if i >= ROWS_TO_SCAN:\n",
    "        break\n",
    "print(f'Rows to process: {len(texts)}')\n",
    "\n",
    "del training_data\n",
    "\n",
    "# Use char to int mapping\n",
    "char_to_int = map_char_to_int(texts)\n",
    "sequences = texts_to_sequences(texts, char_to_int)\n",
    "\n",
    "del texts\n",
    "\n",
    "# dump the parameters into a json file\n",
    "with open(MODEL_PATH + '/params.json', 'w') as params:\n",
    "    json.dump({\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'num_rows_used': len(sequences),\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'char_to_int': char_to_int,\n",
    "        'labels_index': labels_index\n",
    "    }, params)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)\n",
    "del sequences  \n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation split\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Use a 2% validation split size if rows are more than a million\n",
    "\n",
    "validation_ratio = 0.2 if data.shape[0] < (10**6) else 0.02\n",
    "num_validation_samples = int(validation_ratio * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "del data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 4377141\n",
      "Validation Set Size: 89329\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Set Size: {x_train.shape[0]}')\n",
    "print(f'Validation Set Size: {x_val.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0626 08:15:40.696341  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0626 08:15:41.697562  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0626 08:15:41.941638  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0626 08:15:42.466052  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0626 08:15:42.522637  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0626 08:15:42.536451  3400 deprecation.py:506] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0626 08:15:43.018275  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0626 08:15:43.047935  3400 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 16)           1120      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 1024)         82944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 1024)         4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 64, 1024)          5243904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 1024)          5243904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16, 1024)          5243904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8, 1024)           5243904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 69)                70725     \n",
      "=================================================================\n",
      "Total params: 22,204,581\n",
      "Trainable params: 22,192,293\n",
      "Non-trainable params: 12,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_to_int) + 1, EMBEDDING_DIM, input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we only keep the weights from the epoch with the best accuracy, rather than the last set of weights\n",
    "checkpoint_handler = ModelCheckpoint(filepath=MODEL_PATH + '/model.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 08:21:28.656315  3400 deprecation.py:323] From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4377141 samples, validate on 89329 samples\n",
      "Epoch 1/48\n",
      "   2816/4377141 [..............................] - ETA: 93:35:12 - loss: 5.0223 - acc: 0.0384"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[checkpoint_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
