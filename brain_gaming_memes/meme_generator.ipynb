{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Dropout, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import Sequential\n",
    "import util\n",
    "\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.path.realpath(__file__))\n",
    "MODEL_NAME = 'meme_text_gen'\n",
    "MODEL_PATH = util.get_model_path(BASE_PATH, MODEL_NAME)\n",
    "os.mkdir(MODEL_PATH)\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 16\n",
    "ROWS_TO_SCAN = 2000000\n",
    "NUM_EPOCHS = 48\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "print('loading json data...')\n",
    "t = time.time()\n",
    "\n",
    "training_data = json.load(open(BASE_PATH + '/train_gena_24_4000.json'))\n",
    "\n",
    "print('loading json took %ds' % round(time.time() - t))\n",
    "util.print_memory()\n",
    "\n",
    "\n",
    "print('scanning %d of %d json rows...' % (min(ROWS_TO_SCAN, len(training_data)), len(training_data)))\n",
    "t = time.time()\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "label_id_counter = 0\n",
    "for i, row in enumerate(training_data):\n",
    "    template_id = str(row[0]).zfill(12)\n",
    "    text = row[1].lower()\n",
    "    start_index = len(template_id) + 2 + 1 + 2  # template_id, spaces, box_index, spaces\n",
    "    box_index = 0\n",
    "    for j in range(0, len(text)):\n",
    "        char = text[j]\n",
    "        # note: it is critical that the number of spaces plus len(box_index) is >= the convolution width\n",
    "        texts.append(template_id + '  ' + str(box_index) + '  ' + text[0:j])\n",
    "        if char in labels_index:\n",
    "            label_id = labels_index[char]\n",
    "        else:\n",
    "            label_id = label_id_counter\n",
    "            labels_index[char] = label_id\n",
    "            label_id_counter += 1\n",
    "        labels.append(label_id)\n",
    "        if char == '|':\n",
    "            box_index += 1\n",
    "\n",
    "    if i >= ROWS_TO_SCAN:\n",
    "        break\n",
    "\n",
    "\n",
    "print('training text 0: %s' % texts[0])\n",
    "print('training text 10: %s' % texts[10])\n",
    "print('training text 1000: %s' % texts[1000])\n",
    "print('scanning json took %ds' % round(time.time() - t))\n",
    "util.print_memory()\n",
    "\n",
    "\n",
    "print('tokenizing %d texts...' % len(texts))\n",
    "del training_data  # free memory\n",
    "t = time.time()\n",
    "\n",
    "char_to_int = util.map_char_to_int(texts)\n",
    "sequences = util.texts_to_sequences(texts, char_to_int)\n",
    "del texts  # free memory\n",
    "\n",
    "print('example sequence 10: ', sequences[10])\n",
    "print('tokenizing took %ds' % round(time.time() - t))\n",
    "util.print_memory()\n",
    "\n",
    "\n",
    "print('saving tokenizer and labels to file...')\n",
    "\n",
    "# save tokenizer, label indexes, and parameters so they can be used for predicting later\n",
    "with open(MODEL_PATH + '/params.json', 'w') as handle:\n",
    "    json.dump({\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'num_rows_used': len(sequences),\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'char_to_int': char_to_int,\n",
    "        'labels_index': labels_index\n",
    "    }, handle)\n",
    "\n",
    "print('found %s unique tokens.' % len(char_to_int))\n",
    "\n",
    "print('padding sequences...')\n",
    "t = time.time()\n",
    "data = pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)\n",
    "del sequences  # free memory\n",
    "labels = np.asarray(labels)\n",
    "print('padding sequences took %ds' % round(time.time() - t))\n",
    "util.print_memory()\n",
    "\n",
    "print('data:', data)\n",
    "print('labels:', labels)\n",
    "print('shape of data tensor:', data.shape)\n",
    "print('shape of label tensor:', labels.shape)\n",
    "\n",
    "# split data into training and validation\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# validation set can be much smaller if we use a lot of data (source: andrew ng on coursera video)\n",
    "validation_ratio = 0.2 if data.shape[0] < 1000000 else 0.02\n",
    "num_validation_samples = int(validation_ratio * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "del data, labels  # free memory\n",
    "\n",
    "util.print_memory()\n",
    "\n",
    "print('training model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_to_int) + 1, EMBEDDING_DIM, input_length=SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "print('model summary: ')\n",
    "model.summary()\n",
    "# save model summary in model folder so we can reference it later when comparing models\n",
    "with open(MODEL_PATH + '/summary.txt', 'w') as handle:\n",
    "    model.summary(print_fn=lambda x: handle.write(x + '\\n'))\n",
    "\n",
    "# make sure we only keep the weights from the epoch with the best accuracy, rather than the last set of weights\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_PATH + '/model.h5', verbose=1, save_best_only=True)\n",
    "history_checkpointer = util.SaveHistoryCheckpoint(model_path=MODEL_PATH)\n",
    "\n",
    "util.print_memory()\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[checkpointer, history_checkpointer])\n",
    "\n",
    "util.copy_model_to_latest(BASE_PATH, MODEL_PATH, MODEL_NAME)\n",
    "\n",
    "print('total time: %ds' % round(util.total_time()))\n",
    "util.print_memory()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
